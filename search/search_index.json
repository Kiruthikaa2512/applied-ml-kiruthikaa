{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Applied ML Projects \u00b6 Author: Kiruthikaa Natarajan Srinivasan This repository contains applied machine learning projects using a modern Python workflow. Each project follows a professional structure with reproducible environments, documented analysis steps, and clear results. Projects Index \u00b6 Project 01: California Housing Price Prediction Explores regression modeling and feature engineering using the California housing dataset. Project 02: Titanic + Breast Cancer Classification Documents the setup, workflow, and hosting strategy for classification modeling using Titanic and Breast Cancer datasets.","title":"Home"},{"location":"#applied-ml-projects","text":"Author: Kiruthikaa Natarajan Srinivasan This repository contains applied machine learning projects using a modern Python workflow. Each project follows a professional structure with reproducible environments, documented analysis steps, and clear results.","title":"Applied ML Projects"},{"location":"#projects-index","text":"Project 01: California Housing Price Prediction Explores regression modeling and feature engineering using the California housing dataset. Project 02: Titanic + Breast Cancer Classification Documents the setup, workflow, and hosting strategy for classification modeling using Titanic and Breast Cancer datasets.","title":"Projects Index"},{"location":"rebuild/","text":"","title":"Rebuild"},{"location":"project01/","text":"Project 01: Kiruthikaa's California Housing Price Prediction \u00b6 Author : Kiruthikaa Natarajan Srinivasan Date : October 15, 2025 Notebook: You can view the full Jupyter notebook for this project here:https://github.com/Kiruthikaa2512/applied-ml-kiruthikaa/blob/main/notebooks/project01/kiruthikaa_ml01.ipynb # Project 01 \u2014 California Housing Price Prediction Welcome to the **California Housing ML Project** documentation. This project demonstrates a workflow for **data exploration** , **feature selection** , and **linear regression modeling** using the **California Housing dataset** from `scikit-learn` . --- ## Project Overview ### Objective Predict median home values across California districts based on income and housing features. ### Dataset Summary | Feature | Description | |----------|--------------| | `MedInc` | Median income in block group | | `HouseAge` | Median house age | | `AveRooms` | Average number of rooms | | `AveBedrms` | Average number of bedrooms | | `Population` | Block group population | | `AveOccup` | Average household occupancy | | `Latitude` | Block group latitude | | `Longitude` | Block group longitude | | **Target:** `MedHouseVal` | Median house value (in $100,000s) | ## Key Steps 1. **Import and Inspect the Data** - Load dataset using `fetch_california_housing` . - Examine data structure and summary statistics. 2. **Explore the Data** - Visualize distributions with histograms. - Use optional boxenplots and pairplots for in-depth relationships. 3. **Feature Selection** - Focused on key predictors: `MedInc` and `AveRooms` . 4. **Model Training** - Linear Regression using Scikit-Learn. - Evaluate model with `R\u00b2` , `MAE` , and `RMSE` . 5. **Findings** - Median income has the strongest correlation with housing prices. - Achieved **R\u00b2 \u2248 0.46** , showing moderate explanatory power. ## System Setup and Execution ### 1. Clone the Repository ```bash git clone https://github.com/<your-username>/ml-projects.git cd ml-projects/project02 ```` ### 2. Create a Virtual Environment ```bash python -m venv .venv source .venv/bin/activate # Windows: .venv\\Scripts\\activate 3. Install Requirements \u00b6 pip install -r requirements.txt 4. Run the Notebooks \u00b6 jupyter notebook kiruthikaa_ml01.ipynb","title":"Overview"},{"location":"project01/#project-01-kiruthikaas-california-housing-price-prediction","text":"Author : Kiruthikaa Natarajan Srinivasan Date : October 15, 2025 Notebook: You can view the full Jupyter notebook for this project here:https://github.com/Kiruthikaa2512/applied-ml-kiruthikaa/blob/main/notebooks/project01/kiruthikaa_ml01.ipynb # Project 01 \u2014 California Housing Price Prediction Welcome to the **California Housing ML Project** documentation. This project demonstrates a workflow for **data exploration** , **feature selection** , and **linear regression modeling** using the **California Housing dataset** from `scikit-learn` . --- ## Project Overview ### Objective Predict median home values across California districts based on income and housing features. ### Dataset Summary | Feature | Description | |----------|--------------| | `MedInc` | Median income in block group | | `HouseAge` | Median house age | | `AveRooms` | Average number of rooms | | `AveBedrms` | Average number of bedrooms | | `Population` | Block group population | | `AveOccup` | Average household occupancy | | `Latitude` | Block group latitude | | `Longitude` | Block group longitude | | **Target:** `MedHouseVal` | Median house value (in $100,000s) | ## Key Steps 1. **Import and Inspect the Data** - Load dataset using `fetch_california_housing` . - Examine data structure and summary statistics. 2. **Explore the Data** - Visualize distributions with histograms. - Use optional boxenplots and pairplots for in-depth relationships. 3. **Feature Selection** - Focused on key predictors: `MedInc` and `AveRooms` . 4. **Model Training** - Linear Regression using Scikit-Learn. - Evaluate model with `R\u00b2` , `MAE` , and `RMSE` . 5. **Findings** - Median income has the strongest correlation with housing prices. - Achieved **R\u00b2 \u2248 0.46** , showing moderate explanatory power. ## System Setup and Execution ### 1. Clone the Repository ```bash git clone https://github.com/<your-username>/ml-projects.git cd ml-projects/project02 ```` ### 2. Create a Virtual Environment ```bash python -m venv .venv source .venv/bin/activate # Windows: .venv\\Scripts\\activate","title":"Project 01: Kiruthikaa's California Housing Price Prediction"},{"location":"project01/#3-install-requirements","text":"pip install -r requirements.txt","title":"3. Install Requirements"},{"location":"project01/#4-run-the-notebooks","text":"jupyter notebook kiruthikaa_ml01.ipynb","title":"4. Run the Notebooks"},{"location":"project01/EXAMPLE_ANALYSIS/","text":"Project 1 - Analysis \u00b6 Your content here...","title":"Project 1 - Analysis"},{"location":"project01/EXAMPLE_ANALYSIS/#project-1-analysis","text":"Your content here...","title":"Project 1 - Analysis"},{"location":"project02/","text":"Project 02 \u2014 Kiruthikaa's Titanic Survival Prrediction + Breast Cancer Classification \u00b6 Author: Kiruthikaa Natarajan Srinivasan Date: October 29, 2025 Notebook You can view the full Jupyter notebook for this project here: View Full Notebook Overview \u00b6 This project investigates two classic machine learning classification problems: Titanic Survival Prediction \u2014 analyzing demographic and ticket-related features to predict passenger survival. Breast Cancer Classification \u2014 using medical diagnostic data to classify tumors as malignant or benign. Both datasets are used to demonstrate systematic data exploration, cleaning, feature engineering, model training, and evaluation \u2014 with a focus on reproducibility , interpretability , and balanced sampling . \u00b6 Datasets \u00b6 Titanic Dataset \u00b6 Source: seaborn.load_dataset(\"titanic\") Samples: 891 Type: Mixed (numeric + categorical) Target: survived (0 = No, 1 = Yes) Key Features: pclass , sex , age , fare , sibsp , parch , embarked Handling Missing Values - age , deck , and embark_town contain missing entries. - Missing values are imputed using the median (for numeric) and mode (for categorical). Breast Cancer Dataset \u00b6 Source: sklearn.datasets.load_breast_cancer() Samples: 569 Features: 30 numeric diagnostic attributes Target: Binary (0 = malignant, 1 = benign) Goal: Classify tumors based on measurements such as mean radius, texture, smoothness, and symmetry. Project Structure \u00b6 Section Description 1. Import and Inspect the Data Load Titanic and Breast Cancer datasets; inspect types, shapes, and missing values. 2. Data Exploration and Preparation Visualize distributions and correlations; handle nulls and outliers. 3. Feature Engineering Create new features (e.g., family_size , alone ), encode categorical data. 4. Train/Test Split and Model Training Split using both basic and stratified sampling; train baseline classifiers. 5. Bonus: Breast Cancer Classification Apply Logistic Regression and Decision Tree models; compare metrics. 6. Final Interpretation Reflect on results, feature importance, and lessons learned. Titanic Dataset Summary \u00b6 Split Type Non-Survivors Survivors Original 61.6% 38.4% Stratified Train 61.7% 38.3% Stratified Test 61.5% 38.5% Stratified sampling ensures balanced class proportions. Key predictors: sex , pclass , age , and fare . Insights: Female passengers had higher survival rates. Higher-class passengers (1st > 2nd > 3rd) had better survival outcomes. Younger passengers and those with small families fared better. Bonus: \u00b6 Breast Cancer Model Summary \u00b6 Model Accuracy Precision Recall F1-Score Logistic Regression ~96% 0.97 0.96 0.96 Decision Tree ~93% 0.94 0.93 0.93 Key Observations - Logistic Regression provides smoother decision boundaries and better generalization. - Decision Tree offers interpretability through feature importance visualization. - Important features include mean radius , worst area , and mean concave points . System Setup and Execution \u00b6 1. Clone the Repository \u00b6 git clone https://github.com/<your-username>/ml-projects.git cd ml-projects/project02 ```` ### 2. Create a Virtual Environment ``` bash python -m venv .venv source .venv/bin/activate # Windows: .venv\\Scripts\\activate 3. Install Requirements \u00b6 pip install -r requirements.txt 4. Run the Notebooks \u00b6 jupyter notebook ml02_titanic.ipynb jupyter notebook ml02_breast_cancer.ipynb 5. (Optional) Convert to HTML for Hosting \u00b6 jupyter nbconvert --to html --TemplateExporter.exclude_input = True ml02_titanic.ipynb jupyter nbconvert --to html --TemplateExporter.exclude_input = True ml02_breast_cancer.ipynb Requirements \u00b6 pandas numpy seaborn matplotlib scikit-learn Environment Management: The project uses venv (or uv ) for isolation and nbconvert for clean HTML exports. Key Learnings \u00b6 Data balance is critical for fair model evaluation. Feature engineering (e.g., family size, categorical encoding) improves model interpretability. Logistic Regression often outperforms tree-based methods on small, clean datasets. Future Enhancements \u00b6 Add hyperparameter tuning (GridSearchCV / RandomizedSearchCV). Introduce cross-validation for more robust performance estimates. Apply feature scaling and PCA for dimensionality reduction. Extend to ensemble methods (Random Forest, Gradient Boosting). Hosting Strategy \u00b6 The notebooks can be hosted as HTML documentation using MkDocs or GitHub Pages . Example workflow: jupyter nbconvert --to html --TemplateExporter.exclude_input = True docs/project02/ml02_kiruthikaa.ipynb Integrate the generated files in your MkDocs structure: nav : - Home : index.md - Project 01 : project01/index.md - Project 02 : project02/index.md Conclusion \u00b6 Project 02 combines data exploration (Titanic) and classification modeling (Breast Cancer) to illustrate a cohesive end-to-end ML workflow. It demonstrates clean data handling , feature construction , and balanced evaluation \u2014 key foundations for reliable machine learning analysis. ```","title":"Overview"},{"location":"project02/#project-02-kiruthikaas-titanic-survival-prrediction-breast-cancer-classification","text":"Author: Kiruthikaa Natarajan Srinivasan Date: October 29, 2025 Notebook You can view the full Jupyter notebook for this project here: View Full Notebook","title":"Project 02 \u2014 Kiruthikaa's Titanic Survival Prrediction + Breast Cancer Classification"},{"location":"project02/#overview","text":"This project investigates two classic machine learning classification problems: Titanic Survival Prediction \u2014 analyzing demographic and ticket-related features to predict passenger survival. Breast Cancer Classification \u2014 using medical diagnostic data to classify tumors as malignant or benign.","title":"Overview"},{"location":"project02/#both-datasets-are-used-to-demonstrate-systematic-data-exploration-cleaning-feature-engineering-model-training-and-evaluation-with-a-focus-on-reproducibility-interpretability-and-balanced-sampling","text":"","title":"Both datasets are used to demonstrate systematic data exploration, cleaning, feature engineering, model training, and evaluation \u2014 with a focus on reproducibility, interpretability, and balanced sampling."},{"location":"project02/#datasets","text":"","title":"Datasets"},{"location":"project02/#titanic-dataset","text":"Source: seaborn.load_dataset(\"titanic\") Samples: 891 Type: Mixed (numeric + categorical) Target: survived (0 = No, 1 = Yes) Key Features: pclass , sex , age , fare , sibsp , parch , embarked Handling Missing Values - age , deck , and embark_town contain missing entries. - Missing values are imputed using the median (for numeric) and mode (for categorical).","title":"Titanic Dataset"},{"location":"project02/#breast-cancer-dataset","text":"Source: sklearn.datasets.load_breast_cancer() Samples: 569 Features: 30 numeric diagnostic attributes Target: Binary (0 = malignant, 1 = benign) Goal: Classify tumors based on measurements such as mean radius, texture, smoothness, and symmetry.","title":"Breast Cancer Dataset"},{"location":"project02/#project-structure","text":"Section Description 1. Import and Inspect the Data Load Titanic and Breast Cancer datasets; inspect types, shapes, and missing values. 2. Data Exploration and Preparation Visualize distributions and correlations; handle nulls and outliers. 3. Feature Engineering Create new features (e.g., family_size , alone ), encode categorical data. 4. Train/Test Split and Model Training Split using both basic and stratified sampling; train baseline classifiers. 5. Bonus: Breast Cancer Classification Apply Logistic Regression and Decision Tree models; compare metrics. 6. Final Interpretation Reflect on results, feature importance, and lessons learned.","title":"Project Structure"},{"location":"project02/#titanic-dataset-summary","text":"Split Type Non-Survivors Survivors Original 61.6% 38.4% Stratified Train 61.7% 38.3% Stratified Test 61.5% 38.5% Stratified sampling ensures balanced class proportions. Key predictors: sex , pclass , age , and fare . Insights: Female passengers had higher survival rates. Higher-class passengers (1st > 2nd > 3rd) had better survival outcomes. Younger passengers and those with small families fared better.","title":"Titanic Dataset Summary"},{"location":"project02/#bonus","text":"","title":"Bonus:"},{"location":"project02/#breast-cancer-model-summary","text":"Model Accuracy Precision Recall F1-Score Logistic Regression ~96% 0.97 0.96 0.96 Decision Tree ~93% 0.94 0.93 0.93 Key Observations - Logistic Regression provides smoother decision boundaries and better generalization. - Decision Tree offers interpretability through feature importance visualization. - Important features include mean radius , worst area , and mean concave points .","title":"Breast Cancer Model Summary"},{"location":"project02/#system-setup-and-execution","text":"","title":"System Setup and Execution"},{"location":"project02/#1-clone-the-repository","text":"git clone https://github.com/<your-username>/ml-projects.git cd ml-projects/project02 ```` ### 2. Create a Virtual Environment ``` bash python -m venv .venv source .venv/bin/activate # Windows: .venv\\Scripts\\activate","title":"1. Clone the Repository"},{"location":"project02/#3-install-requirements","text":"pip install -r requirements.txt","title":"3. Install Requirements"},{"location":"project02/#4-run-the-notebooks","text":"jupyter notebook ml02_titanic.ipynb jupyter notebook ml02_breast_cancer.ipynb","title":"4. Run the Notebooks"},{"location":"project02/#5-optional-convert-to-html-for-hosting","text":"jupyter nbconvert --to html --TemplateExporter.exclude_input = True ml02_titanic.ipynb jupyter nbconvert --to html --TemplateExporter.exclude_input = True ml02_breast_cancer.ipynb","title":"5. (Optional) Convert to HTML for Hosting"},{"location":"project02/#requirements","text":"pandas numpy seaborn matplotlib scikit-learn Environment Management: The project uses venv (or uv ) for isolation and nbconvert for clean HTML exports.","title":"Requirements"},{"location":"project02/#key-learnings","text":"Data balance is critical for fair model evaluation. Feature engineering (e.g., family size, categorical encoding) improves model interpretability. Logistic Regression often outperforms tree-based methods on small, clean datasets.","title":"Key Learnings"},{"location":"project02/#future-enhancements","text":"Add hyperparameter tuning (GridSearchCV / RandomizedSearchCV). Introduce cross-validation for more robust performance estimates. Apply feature scaling and PCA for dimensionality reduction. Extend to ensemble methods (Random Forest, Gradient Boosting).","title":"Future Enhancements"},{"location":"project02/#hosting-strategy","text":"The notebooks can be hosted as HTML documentation using MkDocs or GitHub Pages . Example workflow: jupyter nbconvert --to html --TemplateExporter.exclude_input = True docs/project02/ml02_kiruthikaa.ipynb Integrate the generated files in your MkDocs structure: nav : - Home : index.md - Project 01 : project01/index.md - Project 02 : project02/index.md","title":"Hosting Strategy"},{"location":"project02/#conclusion","text":"Project 02 combines data exploration (Titanic) and classification modeling (Breast Cancer) to illustrate a cohesive end-to-end ML workflow. It demonstrates clean data handling , feature construction , and balanced evaluation \u2014 key foundations for reliable machine learning analysis. ```","title":"Conclusion"},{"location":"project03/","text":"","title":"Index"},{"location":"project04/","text":"Project 04 \u00b6","title":"Project 04"},{"location":"project04/#project-04","text":"","title":"Project 04"}]}