{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Applied ML Projects \u00b6 Author: Kiruthikaa Natarajan Srinivasan This repository contains applied machine learning projects using a modern Python workflow. Each project follows a professional structure with reproducible environments, documented analysis steps, and clear results. Projects Index \u00b6 Project 01: California Housing Price Prediction Explores regression modeling and feature engineering using the California housing dataset. Project 02: Titanic + Breast Cancer Classification Documents the setup, workflow, and hosting strategy for classification modeling using Titanic and Breast Cancer datasets.","title":"Home"},{"location":"#applied-ml-projects","text":"Author: Kiruthikaa Natarajan Srinivasan This repository contains applied machine learning projects using a modern Python workflow. Each project follows a professional structure with reproducible environments, documented analysis steps, and clear results.","title":"Applied ML Projects"},{"location":"#projects-index","text":"Project 01: California Housing Price Prediction Explores regression modeling and feature engineering using the California housing dataset. Project 02: Titanic + Breast Cancer Classification Documents the setup, workflow, and hosting strategy for classification modeling using Titanic and Breast Cancer datasets.","title":"Projects Index"},{"location":"rebuild/","text":"","title":"Rebuild"},{"location":"project01/","text":"Project 01: Kiruthikaa's California Housing Price Prediction \u00b6 Author : Kiruthikaa Natarajan Srinivasan Date : October 15, 2025 Notebook: You can view the full Jupyter notebook for this project here:https://github.com/Kiruthikaa2512/applied-ml-kiruthikaa/blob/main/notebooks/project01/kiruthikaa_ml01.ipynb # Project 01 \u2014 California Housing Price Prediction Welcome to the **California Housing ML Project** documentation. This project demonstrates a workflow for **data exploration** , **feature selection** , and **linear regression modeling** using the **California Housing dataset** from `scikit-learn` . --- ## Project Overview ### Objective Predict median home values across California districts based on income and housing features. ### Dataset Summary | Feature | Description | |----------|--------------| | `MedInc` | Median income in block group | | `HouseAge` | Median house age | | `AveRooms` | Average number of rooms | | `AveBedrms` | Average number of bedrooms | | `Population` | Block group population | | `AveOccup` | Average household occupancy | | `Latitude` | Block group latitude | | `Longitude` | Block group longitude | | **Target:** `MedHouseVal` | Median house value (in $100,000s) | ## Key Steps 1. **Import and Inspect the Data** - Load dataset using `fetch_california_housing` . - Examine data structure and summary statistics. 2. **Explore the Data** - Visualize distributions with histograms. - Use optional boxenplots and pairplots for in-depth relationships. 3. **Feature Selection** - Focused on key predictors: `MedInc` and `AveRooms` . 4. **Model Training** - Linear Regression using Scikit-Learn. - Evaluate model with `R\u00b2` , `MAE` , and `RMSE` . 5. **Findings** - Median income has the strongest correlation with housing prices. - Achieved **R\u00b2 \u2248 0.46** , showing moderate explanatory power. ## System Setup and Execution ### 1. Clone the Repository ```bash git clone https://github.com/<your-username>/ml-projects.git cd ml-projects/project02 ```` ### 2. Create a Virtual Environment ```bash python -m venv .venv source .venv/bin/activate # Windows: .venv\\Scripts\\activate 3. Install Requirements \u00b6 pip install -r requirements.txt 4. Run the Notebooks \u00b6 jupyter notebook kiruthikaa_ml01.ipynb","title":"Overview"},{"location":"project01/#project-01-kiruthikaas-california-housing-price-prediction","text":"Author : Kiruthikaa Natarajan Srinivasan Date : October 15, 2025 Notebook: You can view the full Jupyter notebook for this project here:https://github.com/Kiruthikaa2512/applied-ml-kiruthikaa/blob/main/notebooks/project01/kiruthikaa_ml01.ipynb # Project 01 \u2014 California Housing Price Prediction Welcome to the **California Housing ML Project** documentation. This project demonstrates a workflow for **data exploration** , **feature selection** , and **linear regression modeling** using the **California Housing dataset** from `scikit-learn` . --- ## Project Overview ### Objective Predict median home values across California districts based on income and housing features. ### Dataset Summary | Feature | Description | |----------|--------------| | `MedInc` | Median income in block group | | `HouseAge` | Median house age | | `AveRooms` | Average number of rooms | | `AveBedrms` | Average number of bedrooms | | `Population` | Block group population | | `AveOccup` | Average household occupancy | | `Latitude` | Block group latitude | | `Longitude` | Block group longitude | | **Target:** `MedHouseVal` | Median house value (in $100,000s) | ## Key Steps 1. **Import and Inspect the Data** - Load dataset using `fetch_california_housing` . - Examine data structure and summary statistics. 2. **Explore the Data** - Visualize distributions with histograms. - Use optional boxenplots and pairplots for in-depth relationships. 3. **Feature Selection** - Focused on key predictors: `MedInc` and `AveRooms` . 4. **Model Training** - Linear Regression using Scikit-Learn. - Evaluate model with `R\u00b2` , `MAE` , and `RMSE` . 5. **Findings** - Median income has the strongest correlation with housing prices. - Achieved **R\u00b2 \u2248 0.46** , showing moderate explanatory power. ## System Setup and Execution ### 1. Clone the Repository ```bash git clone https://github.com/<your-username>/ml-projects.git cd ml-projects/project02 ```` ### 2. Create a Virtual Environment ```bash python -m venv .venv source .venv/bin/activate # Windows: .venv\\Scripts\\activate","title":"Project 01: Kiruthikaa's California Housing Price Prediction"},{"location":"project01/#3-install-requirements","text":"pip install -r requirements.txt","title":"3. Install Requirements"},{"location":"project01/#4-run-the-notebooks","text":"jupyter notebook kiruthikaa_ml01.ipynb","title":"4. Run the Notebooks"},{"location":"project01/EXAMPLE_ANALYSIS/","text":"Project 1 - Analysis \u00b6 Your content here...","title":"Project 1 - Analysis"},{"location":"project01/EXAMPLE_ANALYSIS/#project-1-analysis","text":"Your content here...","title":"Project 1 - Analysis"},{"location":"project02/","text":"Project 02 \u2014 Kiruthikaa's Titanic Survival Prrediction + Breast Cancer Classification \u00b6 Author: Kiruthikaa Natarajan Srinivasan Date: October 29, 2025 Notebook You can view the full Jupyter notebook for this project here: View Full Notebook Overview \u00b6 This project investigates two classic machine learning classification problems: Titanic Survival Prediction \u2014 analyzing demographic and ticket-related features to predict passenger survival. Breast Cancer Classification \u2014 using medical diagnostic data to classify tumors as malignant or benign. Both datasets are used to demonstrate systematic data exploration, cleaning, feature engineering, model training, and evaluation \u2014 with a focus on reproducibility , interpretability , and balanced sampling . \u00b6 Datasets \u00b6 Titanic Dataset \u00b6 Source: seaborn.load_dataset(\"titanic\") Samples: 891 Type: Mixed (numeric + categorical) Target: survived (0 = No, 1 = Yes) Key Features: pclass , sex , age , fare , sibsp , parch , embarked Handling Missing Values - age , deck , and embark_town contain missing entries. - Missing values are imputed using the median (for numeric) and mode (for categorical). Breast Cancer Dataset \u00b6 Source: sklearn.datasets.load_breast_cancer() Samples: 569 Features: 30 numeric diagnostic attributes Target: Binary (0 = malignant, 1 = benign) Goal: Classify tumors based on measurements such as mean radius, texture, smoothness, and symmetry. Project Structure \u00b6 Section Description 1. Import and Inspect the Data Load Titanic and Breast Cancer datasets; inspect types, shapes, and missing values. 2. Data Exploration and Preparation Visualize distributions and correlations; handle nulls and outliers. 3. Feature Engineering Create new features (e.g., family_size , alone ), encode categorical data. 4. Train/Test Split and Model Training Split using both basic and stratified sampling; train baseline classifiers. 5. Bonus: Breast Cancer Classification Apply Logistic Regression and Decision Tree models; compare metrics. 6. Final Interpretation Reflect on results, feature importance, and lessons learned. Titanic Dataset Summary \u00b6 Split Type Non-Survivors Survivors Original 61.6% 38.4% Stratified Train 61.7% 38.3% Stratified Test 61.5% 38.5% Stratified sampling ensures balanced class proportions. Key predictors: sex , pclass , age , and fare . Insights: Female passengers had higher survival rates. Higher-class passengers (1st > 2nd > 3rd) had better survival outcomes. Younger passengers and those with small families fared better. Bonus: \u00b6 Breast Cancer Model Summary \u00b6 Model Accuracy Precision Recall F1-Score Logistic Regression ~96% 0.97 0.96 0.96 Decision Tree ~93% 0.94 0.93 0.93 Key Observations - Logistic Regression provides smoother decision boundaries and better generalization. - Decision Tree offers interpretability through feature importance visualization. - Important features include mean radius , worst area , and mean concave points . System Setup and Execution \u00b6 1. Clone the Repository \u00b6 git clone https://github.com/<your-username>/ml-projects.git cd ml-projects/project02 ```` ### 2. Create a Virtual Environment ``` bash python -m venv .venv source .venv/bin/activate # Windows: .venv\\Scripts\\activate 3. Install Requirements \u00b6 pip install -r requirements.txt 4. Run the Notebooks \u00b6 jupyter notebook ml02_titanic.ipynb jupyter notebook ml02_breast_cancer.ipynb 5. (Optional) Convert to HTML for Hosting \u00b6 jupyter nbconvert --to html --TemplateExporter.exclude_input = True ml02_titanic.ipynb jupyter nbconvert --to html --TemplateExporter.exclude_input = True ml02_breast_cancer.ipynb Requirements \u00b6 pandas numpy seaborn matplotlib scikit-learn Environment Management: The project uses venv (or uv ) for isolation and nbconvert for clean HTML exports. Key Learnings \u00b6 Data balance is critical for fair model evaluation. Feature engineering (e.g., family size, categorical encoding) improves model interpretability. Logistic Regression often outperforms tree-based methods on small, clean datasets. Future Enhancements \u00b6 Add hyperparameter tuning (GridSearchCV / RandomizedSearchCV). Introduce cross-validation for more robust performance estimates. Apply feature scaling and PCA for dimensionality reduction. Extend to ensemble methods (Random Forest, Gradient Boosting). Hosting Strategy \u00b6 The notebooks can be hosted as HTML documentation using MkDocs or GitHub Pages . Example workflow: jupyter nbconvert --to html --TemplateExporter.exclude_input = True docs/project02/ml02_kiruthikaa.ipynb Integrate the generated files in your MkDocs structure: nav : - Home : index.md - Project 01 : project01/index.md - Project 02 : project02/index.md Conclusion \u00b6 Project 02 combines data exploration (Titanic) and classification modeling (Breast Cancer) to illustrate a cohesive end-to-end ML workflow. It demonstrates clean data handling , feature construction , and balanced evaluation \u2014 key foundations for reliable machine learning analysis. ```","title":"Overview"},{"location":"project02/#project-02-kiruthikaas-titanic-survival-prrediction-breast-cancer-classification","text":"Author: Kiruthikaa Natarajan Srinivasan Date: October 29, 2025 Notebook You can view the full Jupyter notebook for this project here: View Full Notebook","title":"Project 02 \u2014 Kiruthikaa's Titanic Survival Prrediction + Breast Cancer Classification"},{"location":"project02/#overview","text":"This project investigates two classic machine learning classification problems: Titanic Survival Prediction \u2014 analyzing demographic and ticket-related features to predict passenger survival. Breast Cancer Classification \u2014 using medical diagnostic data to classify tumors as malignant or benign.","title":"Overview"},{"location":"project02/#both-datasets-are-used-to-demonstrate-systematic-data-exploration-cleaning-feature-engineering-model-training-and-evaluation-with-a-focus-on-reproducibility-interpretability-and-balanced-sampling","text":"","title":"Both datasets are used to demonstrate systematic data exploration, cleaning, feature engineering, model training, and evaluation \u2014 with a focus on reproducibility, interpretability, and balanced sampling."},{"location":"project02/#datasets","text":"","title":"Datasets"},{"location":"project02/#titanic-dataset","text":"Source: seaborn.load_dataset(\"titanic\") Samples: 891 Type: Mixed (numeric + categorical) Target: survived (0 = No, 1 = Yes) Key Features: pclass , sex , age , fare , sibsp , parch , embarked Handling Missing Values - age , deck , and embark_town contain missing entries. - Missing values are imputed using the median (for numeric) and mode (for categorical).","title":"Titanic Dataset"},{"location":"project02/#breast-cancer-dataset","text":"Source: sklearn.datasets.load_breast_cancer() Samples: 569 Features: 30 numeric diagnostic attributes Target: Binary (0 = malignant, 1 = benign) Goal: Classify tumors based on measurements such as mean radius, texture, smoothness, and symmetry.","title":"Breast Cancer Dataset"},{"location":"project02/#project-structure","text":"Section Description 1. Import and Inspect the Data Load Titanic and Breast Cancer datasets; inspect types, shapes, and missing values. 2. Data Exploration and Preparation Visualize distributions and correlations; handle nulls and outliers. 3. Feature Engineering Create new features (e.g., family_size , alone ), encode categorical data. 4. Train/Test Split and Model Training Split using both basic and stratified sampling; train baseline classifiers. 5. Bonus: Breast Cancer Classification Apply Logistic Regression and Decision Tree models; compare metrics. 6. Final Interpretation Reflect on results, feature importance, and lessons learned.","title":"Project Structure"},{"location":"project02/#titanic-dataset-summary","text":"Split Type Non-Survivors Survivors Original 61.6% 38.4% Stratified Train 61.7% 38.3% Stratified Test 61.5% 38.5% Stratified sampling ensures balanced class proportions. Key predictors: sex , pclass , age , and fare . Insights: Female passengers had higher survival rates. Higher-class passengers (1st > 2nd > 3rd) had better survival outcomes. Younger passengers and those with small families fared better.","title":"Titanic Dataset Summary"},{"location":"project02/#bonus","text":"","title":"Bonus:"},{"location":"project02/#breast-cancer-model-summary","text":"Model Accuracy Precision Recall F1-Score Logistic Regression ~96% 0.97 0.96 0.96 Decision Tree ~93% 0.94 0.93 0.93 Key Observations - Logistic Regression provides smoother decision boundaries and better generalization. - Decision Tree offers interpretability through feature importance visualization. - Important features include mean radius , worst area , and mean concave points .","title":"Breast Cancer Model Summary"},{"location":"project02/#system-setup-and-execution","text":"","title":"System Setup and Execution"},{"location":"project02/#1-clone-the-repository","text":"git clone https://github.com/<your-username>/ml-projects.git cd ml-projects/project02 ```` ### 2. Create a Virtual Environment ``` bash python -m venv .venv source .venv/bin/activate # Windows: .venv\\Scripts\\activate","title":"1. Clone the Repository"},{"location":"project02/#3-install-requirements","text":"pip install -r requirements.txt","title":"3. Install Requirements"},{"location":"project02/#4-run-the-notebooks","text":"jupyter notebook ml02_titanic.ipynb jupyter notebook ml02_breast_cancer.ipynb","title":"4. Run the Notebooks"},{"location":"project02/#5-optional-convert-to-html-for-hosting","text":"jupyter nbconvert --to html --TemplateExporter.exclude_input = True ml02_titanic.ipynb jupyter nbconvert --to html --TemplateExporter.exclude_input = True ml02_breast_cancer.ipynb","title":"5. (Optional) Convert to HTML for Hosting"},{"location":"project02/#requirements","text":"pandas numpy seaborn matplotlib scikit-learn Environment Management: The project uses venv (or uv ) for isolation and nbconvert for clean HTML exports.","title":"Requirements"},{"location":"project02/#key-learnings","text":"Data balance is critical for fair model evaluation. Feature engineering (e.g., family size, categorical encoding) improves model interpretability. Logistic Regression often outperforms tree-based methods on small, clean datasets.","title":"Key Learnings"},{"location":"project02/#future-enhancements","text":"Add hyperparameter tuning (GridSearchCV / RandomizedSearchCV). Introduce cross-validation for more robust performance estimates. Apply feature scaling and PCA for dimensionality reduction. Extend to ensemble methods (Random Forest, Gradient Boosting).","title":"Future Enhancements"},{"location":"project02/#hosting-strategy","text":"The notebooks can be hosted as HTML documentation using MkDocs or GitHub Pages . Example workflow: jupyter nbconvert --to html --TemplateExporter.exclude_input = True docs/project02/ml02_kiruthikaa.ipynb Integrate the generated files in your MkDocs structure: nav : - Home : index.md - Project 01 : project01/index.md - Project 02 : project02/index.md","title":"Hosting Strategy"},{"location":"project02/#conclusion","text":"Project 02 combines data exploration (Titanic) and classification modeling (Breast Cancer) to illustrate a cohesive end-to-end ML workflow. It demonstrates clean data handling , feature construction , and balanced evaluation \u2014 key foundations for reliable machine learning analysis. ```","title":"Conclusion"},{"location":"project03/","text":"Project 03 \u2014 Kiruthikaa\u2019s Titanic Classifier + Breast Cancer Bonus \u00b6 Author: Kiruthikaa Natarajan Srinivasan Date: November 4, 2025 Notebook You can view the full Jupyter notebook for this project here: View Full Notebook Overview \u00b6 This project explores two classification tasks: Titanic Survival Prediction \u2014 using social and demographic features to predict passenger survival. Breast Cancer Classification \u2014 applying machine learning to medical diagnostic data to classify tumors. The focus is on comparing multiple models (Decision Tree, SVM, Neural Network), evaluating feature impact, and visualizing decision boundaries. The bonus experiment tests model generalization on a well-separated dataset. Datasets \u00b6 Titanic Dataset \u00b6 Source: seaborn.load_dataset(\"titanic\") Samples: 891 Target: survived (0 = No, 1 = Yes) Key Features Used: alone , age , family_size (engineered from sibsp + parch ) Handling Missing Values - age and embark_town were imputed using median and mode respectively. Breast Cancer Dataset \u00b6 Source: sklearn.datasets.load_breast_cancer() Samples: 569 Features: 30 numeric diagnostic attributes Target: Binary (0 = malignant, 1 = benign) Project Structure \u00b6 Section Description 1. Data Preparation Clean missing values, engineer family_size , encode categorical features 2. Feature Selection Compare three cases: alone , age , age + family_size 3. Decision Tree Models Train and evaluate across all cases; visualize trees and confusion matrices 4. SVM Models Compare kernels (RBF, linear, poly, sigmoid); visualize support vectors 5. Neural Network Train MLP on Case 3; visualize decision surface 6. Bonus: Breast Cancer Classification Apply all models to medical dataset; compare metrics 7. Final Reflections Discuss model behavior, feature impact, and next steps Titanic Model Comparison (Test Set) \u00b6 Model Type Case Features Used Accuracy Precision Recall F1-Score Notes Decision Tree 1 alone 63% 64% 63% 63% Balanced baseline Decision Tree 2 age 61% 58% 61% 55% Weak recall for survivors Decision Tree 3 age + family_size 59% 57% 59% 57% Overfitting observed SVC (RBF) 1 alone 63% 64% 63% 63% Balanced SVC (RBF) 2 age 63% 66% 63% 52% High precision, poor recall SVC (RBF) 3 age + family_size 63% 66% 63% 52% No gain from extra feature SVC (Linear) 3 age + family_size 61% 38% 61% 47% Predicted only non-survivors SVC (Poly) 3 age + family_size 61% 38% 61% 47% Similar to linear SVC (Sigmoid) 3 age + family_size 55% 55% 55% 55% More balanced, lower accuracy Neural Network 3 age + family_size 66% 65% 66% 65% Best overall performance Breast Cancer Model Summary \u00b6 Model Accuracy Precision Recall F1-Score Decision Tree 91% 90% 92% 91% SVC (RBF) 93% 93% 91% 93% Neural Network 94% 94% 93% 94% Key Observations - All models performed well due to clear class separation. - Neural Network had the best balance and generalization. - Feature scaling could further improve SVC and MLP performance. System Setup and Execution \u00b6 1. Clone the Repository \u00b6 git clone https://github.com/Kiruthikaa2512/applied-ml-kiruthikaa.git cd applied-ml-kiruthikaa/notebooks/project03 2. Create a Virtual Environment \u00b6 python -m venv .venv source .venv/bin/activate # Windows: .venv\\Scripts\\activate 3. Install Requirements \u00b6 pip install -r requirements.txt 4. Run the Notebook \u00b6 jupyter notebook ml03_kiruthikaa.ipynb 5. Convert to HTML for Hosting \u00b6 jupyter nbconvert --to html --TemplateExporter.exclude_input = True ml03_kiruthikaa.ipynb Requirements \u00b6 pandas numpy seaborn matplotlib scikit-learn Key Learnings \u00b6 Simple features like alone can outperform more complex combinations. Class imbalance affects precision and recall \u2014 especially in SVM. Neural networks offer smoother decision boundaries but require tuning. Visualizations reveal model behavior beyond raw metrics. Future Enhancements \u00b6 Add hyperparameter tuning (GridSearchCV) Use cross-validation for reliability Include more features ( pclass , sex , fare ) Try ensemble methods (Random Forest, Gradient Boosting)","title":"Overview"},{"location":"project03/#project-03-kiruthikaas-titanic-classifier-breast-cancer-bonus","text":"Author: Kiruthikaa Natarajan Srinivasan Date: November 4, 2025 Notebook You can view the full Jupyter notebook for this project here: View Full Notebook","title":"Project 03 \u2014 Kiruthikaa\u2019s Titanic Classifier + Breast Cancer Bonus"},{"location":"project03/#overview","text":"This project explores two classification tasks: Titanic Survival Prediction \u2014 using social and demographic features to predict passenger survival. Breast Cancer Classification \u2014 applying machine learning to medical diagnostic data to classify tumors. The focus is on comparing multiple models (Decision Tree, SVM, Neural Network), evaluating feature impact, and visualizing decision boundaries. The bonus experiment tests model generalization on a well-separated dataset.","title":"Overview"},{"location":"project03/#datasets","text":"","title":"Datasets"},{"location":"project03/#titanic-dataset","text":"Source: seaborn.load_dataset(\"titanic\") Samples: 891 Target: survived (0 = No, 1 = Yes) Key Features Used: alone , age , family_size (engineered from sibsp + parch ) Handling Missing Values - age and embark_town were imputed using median and mode respectively.","title":"Titanic Dataset"},{"location":"project03/#breast-cancer-dataset","text":"Source: sklearn.datasets.load_breast_cancer() Samples: 569 Features: 30 numeric diagnostic attributes Target: Binary (0 = malignant, 1 = benign)","title":"Breast Cancer Dataset"},{"location":"project03/#project-structure","text":"Section Description 1. Data Preparation Clean missing values, engineer family_size , encode categorical features 2. Feature Selection Compare three cases: alone , age , age + family_size 3. Decision Tree Models Train and evaluate across all cases; visualize trees and confusion matrices 4. SVM Models Compare kernels (RBF, linear, poly, sigmoid); visualize support vectors 5. Neural Network Train MLP on Case 3; visualize decision surface 6. Bonus: Breast Cancer Classification Apply all models to medical dataset; compare metrics 7. Final Reflections Discuss model behavior, feature impact, and next steps","title":"Project Structure"},{"location":"project03/#titanic-model-comparison-test-set","text":"Model Type Case Features Used Accuracy Precision Recall F1-Score Notes Decision Tree 1 alone 63% 64% 63% 63% Balanced baseline Decision Tree 2 age 61% 58% 61% 55% Weak recall for survivors Decision Tree 3 age + family_size 59% 57% 59% 57% Overfitting observed SVC (RBF) 1 alone 63% 64% 63% 63% Balanced SVC (RBF) 2 age 63% 66% 63% 52% High precision, poor recall SVC (RBF) 3 age + family_size 63% 66% 63% 52% No gain from extra feature SVC (Linear) 3 age + family_size 61% 38% 61% 47% Predicted only non-survivors SVC (Poly) 3 age + family_size 61% 38% 61% 47% Similar to linear SVC (Sigmoid) 3 age + family_size 55% 55% 55% 55% More balanced, lower accuracy Neural Network 3 age + family_size 66% 65% 66% 65% Best overall performance","title":"Titanic Model Comparison (Test Set)"},{"location":"project03/#breast-cancer-model-summary","text":"Model Accuracy Precision Recall F1-Score Decision Tree 91% 90% 92% 91% SVC (RBF) 93% 93% 91% 93% Neural Network 94% 94% 93% 94% Key Observations - All models performed well due to clear class separation. - Neural Network had the best balance and generalization. - Feature scaling could further improve SVC and MLP performance.","title":"Breast Cancer Model Summary"},{"location":"project03/#system-setup-and-execution","text":"","title":"System Setup and Execution"},{"location":"project03/#1-clone-the-repository","text":"git clone https://github.com/Kiruthikaa2512/applied-ml-kiruthikaa.git cd applied-ml-kiruthikaa/notebooks/project03","title":"1. Clone the Repository"},{"location":"project03/#2-create-a-virtual-environment","text":"python -m venv .venv source .venv/bin/activate # Windows: .venv\\Scripts\\activate","title":"2. Create a Virtual Environment"},{"location":"project03/#3-install-requirements","text":"pip install -r requirements.txt","title":"3. Install Requirements"},{"location":"project03/#4-run-the-notebook","text":"jupyter notebook ml03_kiruthikaa.ipynb","title":"4. Run the Notebook"},{"location":"project03/#5-convert-to-html-for-hosting","text":"jupyter nbconvert --to html --TemplateExporter.exclude_input = True ml03_kiruthikaa.ipynb","title":"5. Convert to HTML for Hosting"},{"location":"project03/#requirements","text":"pandas numpy seaborn matplotlib scikit-learn","title":"Requirements"},{"location":"project03/#key-learnings","text":"Simple features like alone can outperform more complex combinations. Class imbalance affects precision and recall \u2014 especially in SVM. Neural networks offer smoother decision boundaries but require tuning. Visualizations reveal model behavior beyond raw metrics.","title":"Key Learnings"},{"location":"project03/#future-enhancements","text":"Add hyperparameter tuning (GridSearchCV) Use cross-validation for reliability Include more features ( pclass , sex , fare ) Try ensemble methods (Random Forest, Gradient Boosting)","title":"Future Enhancements"},{"location":"project04/","text":"Project 04 \u2014 Regression with Titanic Fare + California Housing \u00b6 Author: Kiruthikaa Natarajan Srinivasan Date: November 12, 2025 Notebook You can view the full Jupyter notebook for this project here: View Full Notebook Overview \u00b6 This project explores two regression tasks: Titanic Fare Prediction \u2014 estimating ticket prices based on passenger demographics and travel features. California Housing Price Prediction \u2014 modeling median house values using geographic and socioeconomic indicators. The focus is on comparing linear, regularized, and polynomial regression models, evaluating feature impact, and visualizing model performance. The California dataset provides a richer feature set, while Titanic highlights the challenges of sparse inputs. Datasets \u00b6 Titanic Dataset \u00b6 Source: seaborn.load_dataset(\"titanic\") Samples: 891 Target: fare (continuous) Key Features Used: age , family_size , sex_encoded Engineered: family_size = sibsp + parch + 1 Handling Missing Values - age filled with median - embarked filled with \"missing\" and one-hot encoded California Housing Dataset \u00b6 Source: sklearn.datasets.fetch_california_housing() Samples: 20,640 Target: MedHouseVal (median house value in $100,000s) Features: MedInc , AveRooms , AveOccup , Latitude , Longitude , etc. Project Structure \u00b6 Section Description 1. Data Preparation Clean missing values, engineer features, encode categorical variables 2. Feature Selection Compare four cases: age , family_size , age + family_size , sex_encoded 3. Linear Regression Train and evaluate across all cases; inspect coefficients and metrics 4. Model Comparison Ridge, ElasticNet, Polynomial Regression (degree 3 and 8) 5. Visual Diagnostics Predicted vs actual plots, residuals, R\u00b2 bar charts 6. California Housing Train linear regression on full feature set; visualize performance 7. Final Reflections Discuss model behavior, feature impact, and next steps Titanic Regression Results (Test Set) \u00b6 Case Features Used R\u00b2 RMSE MAE Notes 1 age 0.003 37.97 25.29 Weak predictor 2 family_size 0.022 37.61 25.03 Slight improvement 3 age + family_size 0.050 37.08 24.28 Best combined case 4 sex_encoded 0.099 36.10 24.24 Best single feature Alternative Models (Case 3) \u00b6 Model R\u00b2 RMSE MAE Linear Regression 0.050 37.08 24.28 Ridge Regression 0.050 37.08 24.28 ElasticNet 0.054 36.99 24.21 Polynomial (deg 3) 0.064 36.79 23.14 Key Observations - Polynomial regression captured non-linear trends better than linear or regularized models. - Sex alone was surprisingly predictive, likely due to class and cabin correlations. - All models underfit due to limited features \u2014 adding pclass , embarked , or title could improve performance. California Housing Results \u00b6 R\u00b2: 0.608 RMSE: 0.719 (~$71,900) MAE: 0.527 (~$52,700) Visuals Included - Target distribution histogram - Predicted vs actual scatterplot - Residuals vs predicted plot Conclusion: The California model performed significantly better due to richer features and larger sample size. Income and location were key drivers of housing prices. System Setup and Execution \u00b6 1. Clone the Repository \u00b6 git clone https://github.com/Kiruthikaa2512/applied-ml-kiruthikaa.git cd applied-ml-kiruthikaa/notebooks/project04 2. Create a Virtual Environment \u00b6 python -m venv .venv source .venv/bin/activate # Windows: .venv\\Scripts\\activate 3. Install Requirements \u00b6 pip install -r requirements.txt 4. Run the Notebook \u00b6 jupyter notebook ml04_kiruthikaa.ipynb 5. Convert to HTML for Hosting \u00b6 jupyter nbconvert --to html ml04_kiruthikaa.ipynb --output-dir ../../docs --output ml04_kiruthikaa Requirements \u00b6 pandas numpy seaborn matplotlib scikit-learn Key Learnings \u00b6 Regression models require careful feature selection to avoid underfitting. Simple features like sex can be surprisingly predictive. Polynomial regression improves fit but risks overfitting at extremes. Visual diagnostics are essential for interpreting model behavior. Future Enhancements \u00b6 Add pclass , embarked , and interaction terms to Titanic model Try ensemble methods (Random Forest, Gradient Boosting) Apply log transformation or scaling to reduce skew Use cross-validation for more robust evaluation","title":"Overview"},{"location":"project04/#project-04-regression-with-titanic-fare-california-housing","text":"Author: Kiruthikaa Natarajan Srinivasan Date: November 12, 2025 Notebook You can view the full Jupyter notebook for this project here: View Full Notebook","title":"Project 04 \u2014 Regression with Titanic Fare + California Housing"},{"location":"project04/#overview","text":"This project explores two regression tasks: Titanic Fare Prediction \u2014 estimating ticket prices based on passenger demographics and travel features. California Housing Price Prediction \u2014 modeling median house values using geographic and socioeconomic indicators. The focus is on comparing linear, regularized, and polynomial regression models, evaluating feature impact, and visualizing model performance. The California dataset provides a richer feature set, while Titanic highlights the challenges of sparse inputs.","title":"Overview"},{"location":"project04/#datasets","text":"","title":"Datasets"},{"location":"project04/#titanic-dataset","text":"Source: seaborn.load_dataset(\"titanic\") Samples: 891 Target: fare (continuous) Key Features Used: age , family_size , sex_encoded Engineered: family_size = sibsp + parch + 1 Handling Missing Values - age filled with median - embarked filled with \"missing\" and one-hot encoded","title":"Titanic Dataset"},{"location":"project04/#california-housing-dataset","text":"Source: sklearn.datasets.fetch_california_housing() Samples: 20,640 Target: MedHouseVal (median house value in $100,000s) Features: MedInc , AveRooms , AveOccup , Latitude , Longitude , etc.","title":"California Housing Dataset"},{"location":"project04/#project-structure","text":"Section Description 1. Data Preparation Clean missing values, engineer features, encode categorical variables 2. Feature Selection Compare four cases: age , family_size , age + family_size , sex_encoded 3. Linear Regression Train and evaluate across all cases; inspect coefficients and metrics 4. Model Comparison Ridge, ElasticNet, Polynomial Regression (degree 3 and 8) 5. Visual Diagnostics Predicted vs actual plots, residuals, R\u00b2 bar charts 6. California Housing Train linear regression on full feature set; visualize performance 7. Final Reflections Discuss model behavior, feature impact, and next steps","title":"Project Structure"},{"location":"project04/#titanic-regression-results-test-set","text":"Case Features Used R\u00b2 RMSE MAE Notes 1 age 0.003 37.97 25.29 Weak predictor 2 family_size 0.022 37.61 25.03 Slight improvement 3 age + family_size 0.050 37.08 24.28 Best combined case 4 sex_encoded 0.099 36.10 24.24 Best single feature","title":"Titanic Regression Results (Test Set)"},{"location":"project04/#alternative-models-case-3","text":"Model R\u00b2 RMSE MAE Linear Regression 0.050 37.08 24.28 Ridge Regression 0.050 37.08 24.28 ElasticNet 0.054 36.99 24.21 Polynomial (deg 3) 0.064 36.79 23.14 Key Observations - Polynomial regression captured non-linear trends better than linear or regularized models. - Sex alone was surprisingly predictive, likely due to class and cabin correlations. - All models underfit due to limited features \u2014 adding pclass , embarked , or title could improve performance.","title":"Alternative Models (Case 3)"},{"location":"project04/#california-housing-results","text":"R\u00b2: 0.608 RMSE: 0.719 (~$71,900) MAE: 0.527 (~$52,700) Visuals Included - Target distribution histogram - Predicted vs actual scatterplot - Residuals vs predicted plot Conclusion: The California model performed significantly better due to richer features and larger sample size. Income and location were key drivers of housing prices.","title":"California Housing Results"},{"location":"project04/#system-setup-and-execution","text":"","title":"System Setup and Execution"},{"location":"project04/#1-clone-the-repository","text":"git clone https://github.com/Kiruthikaa2512/applied-ml-kiruthikaa.git cd applied-ml-kiruthikaa/notebooks/project04","title":"1. Clone the Repository"},{"location":"project04/#2-create-a-virtual-environment","text":"python -m venv .venv source .venv/bin/activate # Windows: .venv\\Scripts\\activate","title":"2. Create a Virtual Environment"},{"location":"project04/#3-install-requirements","text":"pip install -r requirements.txt","title":"3. Install Requirements"},{"location":"project04/#4-run-the-notebook","text":"jupyter notebook ml04_kiruthikaa.ipynb","title":"4. Run the Notebook"},{"location":"project04/#5-convert-to-html-for-hosting","text":"jupyter nbconvert --to html ml04_kiruthikaa.ipynb --output-dir ../../docs --output ml04_kiruthikaa","title":"5. Convert to HTML for Hosting"},{"location":"project04/#requirements","text":"pandas numpy seaborn matplotlib scikit-learn","title":"Requirements"},{"location":"project04/#key-learnings","text":"Regression models require careful feature selection to avoid underfitting. Simple features like sex can be surprisingly predictive. Polynomial regression improves fit but risks overfitting at extremes. Visual diagnostics are essential for interpreting model behavior.","title":"Key Learnings"},{"location":"project04/#future-enhancements","text":"Add pclass , embarked , and interaction terms to Titanic model Try ensemble methods (Random Forest, Gradient Boosting) Apply log transformation or scaling to reduce skew Use cross-validation for more robust evaluation","title":"Future Enhancements"},{"location":"project05/","text":"Wine Quality Classification with Ensemble Models \u00b6 Author: Kiruthikaa Natarajan Srinivasan Date: November 18, 2025 Notebook You can view the full Jupyter notebook for this project here: View Full Notebook Overview \u00b6 This project applies ensemble machine learning methods to classify red wine quality using data from the UCI Wine Quality dataset. The task is to predict whether a wine falls into the low, medium, or high quality category based on its physicochemical characteristics. The notebook follows a complete workflow from exploring the dataset to preparing features, training models, evaluating results, and comparing model performance. The two main ensemble models used are Random Forest and Gradient Boosting. 1. Understanding the Dataset \u00b6 The dataset includes 1599 samples of red wine and 11 numeric features such as acidity, sulphates, and alcohol content. The original quality score ranges from 0 to 10, but the differences between adjacent scores are small, so I converted the values into three broader categories: Low quality: 3\u20134 Medium quality: 5\u20136 High quality: 7\u20138 These were also mapped to numeric labels (0, 1, 2) for model training. I visualized the distribution of the labels, examined histograms of the quality scores, and used boxplots to show the relationship between each label and the original numeric quality. 2. Preparing the Data \u00b6 The data preparation steps included: Creating the categorical and numeric target variables Selecting all 11 chemical attributes as features Dropping the original quality column Splitting the data into training and testing sets with an 80/20 ratio Using stratified sampling to maintain the class proportions Stratification was particularly important because the dataset is imbalanced, with the medium-quality class dominating. 3. Choosing the Models \u00b6 Two ensemble methods were chosen for evaluation: Random Forest (100 Trees) \u00b6 This model was used as a strong baseline. It works well with numerical features and provides feature importance, which helps with interpretation. Gradient Boosting (100 Estimators) \u00b6 This model builds trees sequentially and tends to handle difficult cases more effectively. It often offers better generalization compared to bagging. Both models were trained using the same helper function to ensure consistency in evaluation. 4. Training and Evaluation \u00b6 Each model was evaluated on: Training accuracy Test accuracy Weighted F1 score Confusion matrices These results helped show not only how well each model performed but also how well it generalized. Additional diagnostics included: Feature importance plot for Random Forest Error vs number of trees plot for Gradient Boosting Confusion matrix heatmaps to visualize misclassifications These visualizations made it easier to interpret each model\u2019s strengths and weaknesses. 5. Comparing Performance \u00b6 I created a consolidated results table containing accuracy, F1 scores, and gap values (difference between training and test metrics). Sorting the table by test accuracy provided a clear view of which model performed best. Key observations: Random Forest achieved the highest accuracy on the test set. Gradient Boosting was slightly lower in accuracy but had smaller gaps, showing more stable generalization. Both models had difficulty predicting the low-quality class because of its small sample size. Alcohol, volatile acidity, and sulphates stood out as important features. 6. Key Insights \u00b6 A few main takeaways from the project: Bagging models like Random Forest can deliver high accuracy but may overfit when trees become highly specialized. Boosting models build on errors and often generalize better on imbalanced data. Class imbalance significantly affects performance, especially on minority classes. High test accuracy does not always mean all classes are being predicted well. Feature importance helps highlight which chemical properties contribute most to wine quality predictions. 7. Next Steps \u00b6 If I continue improving the model, I would explore: Oversampling methods such as SMOTE Hyperparameter tuning for both ensemble models Trying advanced boosting algorithms such as XGBoost or LightGBM Experimenting with probability calibration Adding multiclass ROC curves for deeper evaluation These steps could help improve performance, particularly on the minority classes. Conclusion \u00b6 This project helped me practice applying ensemble models to a real classification problem. Working through the full pipeline\u2014data exploration, feature engineering, model building, evaluation, and comparison\u2014gave me a clearer understanding of how ensemble methods behave and how to interpret their results. The project also emphasized the importance of communicating findings clearly and basing conclusions on data.","title":"Overview"},{"location":"project05/#wine-quality-classification-with-ensemble-models","text":"Author: Kiruthikaa Natarajan Srinivasan Date: November 18, 2025 Notebook You can view the full Jupyter notebook for this project here: View Full Notebook","title":"Wine Quality Classification with Ensemble Models"},{"location":"project05/#overview","text":"This project applies ensemble machine learning methods to classify red wine quality using data from the UCI Wine Quality dataset. The task is to predict whether a wine falls into the low, medium, or high quality category based on its physicochemical characteristics. The notebook follows a complete workflow from exploring the dataset to preparing features, training models, evaluating results, and comparing model performance. The two main ensemble models used are Random Forest and Gradient Boosting.","title":"Overview"},{"location":"project05/#1-understanding-the-dataset","text":"The dataset includes 1599 samples of red wine and 11 numeric features such as acidity, sulphates, and alcohol content. The original quality score ranges from 0 to 10, but the differences between adjacent scores are small, so I converted the values into three broader categories: Low quality: 3\u20134 Medium quality: 5\u20136 High quality: 7\u20138 These were also mapped to numeric labels (0, 1, 2) for model training. I visualized the distribution of the labels, examined histograms of the quality scores, and used boxplots to show the relationship between each label and the original numeric quality.","title":"1. Understanding the Dataset"},{"location":"project05/#2-preparing-the-data","text":"The data preparation steps included: Creating the categorical and numeric target variables Selecting all 11 chemical attributes as features Dropping the original quality column Splitting the data into training and testing sets with an 80/20 ratio Using stratified sampling to maintain the class proportions Stratification was particularly important because the dataset is imbalanced, with the medium-quality class dominating.","title":"2. Preparing the Data"},{"location":"project05/#3-choosing-the-models","text":"Two ensemble methods were chosen for evaluation:","title":"3. Choosing the Models"},{"location":"project05/#random-forest-100-trees","text":"This model was used as a strong baseline. It works well with numerical features and provides feature importance, which helps with interpretation.","title":"Random Forest (100 Trees)"},{"location":"project05/#gradient-boosting-100-estimators","text":"This model builds trees sequentially and tends to handle difficult cases more effectively. It often offers better generalization compared to bagging. Both models were trained using the same helper function to ensure consistency in evaluation.","title":"Gradient Boosting (100 Estimators)"},{"location":"project05/#4-training-and-evaluation","text":"Each model was evaluated on: Training accuracy Test accuracy Weighted F1 score Confusion matrices These results helped show not only how well each model performed but also how well it generalized. Additional diagnostics included: Feature importance plot for Random Forest Error vs number of trees plot for Gradient Boosting Confusion matrix heatmaps to visualize misclassifications These visualizations made it easier to interpret each model\u2019s strengths and weaknesses.","title":"4. Training and Evaluation"},{"location":"project05/#5-comparing-performance","text":"I created a consolidated results table containing accuracy, F1 scores, and gap values (difference between training and test metrics). Sorting the table by test accuracy provided a clear view of which model performed best. Key observations: Random Forest achieved the highest accuracy on the test set. Gradient Boosting was slightly lower in accuracy but had smaller gaps, showing more stable generalization. Both models had difficulty predicting the low-quality class because of its small sample size. Alcohol, volatile acidity, and sulphates stood out as important features.","title":"5. Comparing Performance"},{"location":"project05/#6-key-insights","text":"A few main takeaways from the project: Bagging models like Random Forest can deliver high accuracy but may overfit when trees become highly specialized. Boosting models build on errors and often generalize better on imbalanced data. Class imbalance significantly affects performance, especially on minority classes. High test accuracy does not always mean all classes are being predicted well. Feature importance helps highlight which chemical properties contribute most to wine quality predictions.","title":"6. Key Insights"},{"location":"project05/#7-next-steps","text":"If I continue improving the model, I would explore: Oversampling methods such as SMOTE Hyperparameter tuning for both ensemble models Trying advanced boosting algorithms such as XGBoost or LightGBM Experimenting with probability calibration Adding multiclass ROC curves for deeper evaluation These steps could help improve performance, particularly on the minority classes.","title":"7. Next Steps"},{"location":"project05/#conclusion","text":"This project helped me practice applying ensemble models to a real classification problem. Working through the full pipeline\u2014data exploration, feature engineering, model building, evaluation, and comparison\u2014gave me a clearer understanding of how ensemble methods behave and how to interpret their results. The project also emphasized the importance of communicating findings clearly and basing conclusions on data.","title":"Conclusion"}]}